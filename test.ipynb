{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fcca3b2c850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import ldm.data.laionAE as lae\n",
    "import numpy as np\n",
    "import os\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch\n",
    "import yaml\n",
    "from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, AutoencoderKL, DDIMScheduler\n",
    "from controlnet_aux.midas import MidasDetector\n",
    "from PIL import Image\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_sd_sample_set(\n",
    "        pipe,\n",
    "        ds,\n",
    "        path2samples,\n",
    "        model_version,\n",
    "        n_samples=None,\n",
    "        scale=9.5,\n",
    "        eta=0.5,\n",
    "        ddim_steps=25,\n",
    "        caption_idx=0,\n",
    "        control_scale=1.0,\n",
    "        control_mode='midas',\n",
    "):\n",
    "\n",
    "    idx = 0\n",
    "    seed_everything(42)\n",
    "    sample_save_path = os.path.join(path2samples, model_version,\n",
    "                                    f'steps-{ddim_steps}',\n",
    "                                    f'caption-{caption_idx}'\n",
    "                                    )\n",
    "    if not os.path.exists(sample_save_path):\n",
    "        os.makedirs(sample_save_path)\n",
    "        print(f'[CREATED \\n{sample_save_path} \\n]')\n",
    "\n",
    "    if control_mode == 'midas':\n",
    "        midas_depth = MidasDetector.from_pretrained(\n",
    "            \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\",\n",
    "            cache_dir='/export/scratch/ffeiden/Pretrained%20Originals/T2I_XL/'\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "    while idx < min(len(ds), n_samples):\n",
    "\n",
    "        if control_mode == 'canny':\n",
    "            image = torch.from_numpy(ds[idx]['hint'].copy()).float().permute(2, 0, 1)\n",
    "\n",
    "        elif control_mode == 'midas':\n",
    "            image = Image.fromarray(((ds[idx]['image'] / 2. + 0.5) * 255.).astype(np.uint8))\n",
    "\n",
    "            if not image.mode == \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "\n",
    "            image = midas_depth(\n",
    "                image, detect_resolution=512, image_resolution=512\n",
    "                )\n",
    "\n",
    "        prompt = ds[idx]['caption']\n",
    "        negative_prompt = ''\n",
    "\n",
    "        #### sampling\n",
    "\n",
    "        gen_images = pipe(\n",
    "                        prompt=prompt,\n",
    "                        negative_prompt=negative_prompt,\n",
    "                        image=image,\n",
    "                        num_inference_steps=ddim_steps,\n",
    "                        adapter_conditioning_scale=control_scale,\n",
    "                        guidance_scale=scale,\n",
    "                        ).images[0]\n",
    "\n",
    "        #  gen_images.save(os.path.join(sample_save_path, f'{idx:06}.jpg'))\n",
    "        plt.imshow(gen_images)\n",
    "        plt.show()\n",
    "        if idx >= min(len(ds), n_samples):\n",
    "            break\n",
    "        else:\n",
    "            idx += 1\n",
    "\n",
    "    stats_dict = {\n",
    "        'cfg_scale': scale,\n",
    "        'eta': eta,\n",
    "        'control_scale': control_scale,\n",
    "        'ddim_steps': ddim_steps,\n",
    "        'caption_idx': caption_idx,\n",
    "        'sample_path': sample_save_path\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(sample_save_path, 'stats_dict.yaml'), 'w') as f:\n",
    "        yaml.dump(stats_dict, f, default_flow_style=False)\n",
    "\n",
    "    print(f'[SAMPLES CALCULATED AND SAVED IN\\n{sample_save_path}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m euler_a \u001b[39m=\u001b[39m DDIMScheduler\u001b[39m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mscheduler\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                         cache_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/export/scratch/ffeiden/Pretrained\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Originals/T2I_XL/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m vae \u001b[39m=\u001b[39m AutoencoderKL\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmadebyollin/sdxl-vae-fp16-fix\u001b[39m\u001b[39m\"\u001b[39m, torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m     23\u001b[0m                                     cache_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/export/scratch/ffeiden/Pretrained\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Originals/T2I_XL/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionXLAdapterPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     25\u001b[0m     model_id, vae\u001b[39m=\u001b[39;49mvae, adapter\u001b[39m=\u001b[39;49madapter, scheduler\u001b[39m=\u001b[39;49meuler_a, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m     26\u001b[0m     cache_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/export/scratch/ffeiden/Pretrained\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m20Originals/T2I_XL/\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     27\u001b[0m     )\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m pipe\u001b[39m.\u001b[39menable_xformers_memory_efficient_attention()\n\u001b[1;32m     30\u001b[0m caption_csv_list \u001b[39m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx0.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx1.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     37\u001b[0m                     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SDXL/lib/python3.8/site-packages/diffusers/pipelines/pipeline_utils.py:1095\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     loaded_sub_model \u001b[39m=\u001b[39m passed_class_obj[name]\n\u001b[1;32m   1093\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# load sub model\u001b[39;00m\n\u001b[0;32m-> 1095\u001b[0m     loaded_sub_model \u001b[39m=\u001b[39m load_sub_model(\n\u001b[1;32m   1096\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1097\u001b[0m         class_name\u001b[39m=\u001b[39;49mclass_name,\n\u001b[1;32m   1098\u001b[0m         importable_classes\u001b[39m=\u001b[39;49mimportable_classes,\n\u001b[1;32m   1099\u001b[0m         pipelines\u001b[39m=\u001b[39;49mpipelines,\n\u001b[1;32m   1100\u001b[0m         is_pipeline_module\u001b[39m=\u001b[39;49mis_pipeline_module,\n\u001b[1;32m   1101\u001b[0m         pipeline_class\u001b[39m=\u001b[39;49mpipeline_class,\n\u001b[1;32m   1102\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   1103\u001b[0m         provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m   1104\u001b[0m         sess_options\u001b[39m=\u001b[39;49msess_options,\n\u001b[1;32m   1105\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   1106\u001b[0m         max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m   1107\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   1108\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   1109\u001b[0m         model_variants\u001b[39m=\u001b[39;49mmodel_variants,\n\u001b[1;32m   1110\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1111\u001b[0m         from_flax\u001b[39m=\u001b[39;49mfrom_flax,\n\u001b[1;32m   1112\u001b[0m         variant\u001b[39m=\u001b[39;49mvariant,\n\u001b[1;32m   1113\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   1114\u001b[0m         cached_folder\u001b[39m=\u001b[39;49mcached_folder,\n\u001b[1;32m   1115\u001b[0m     )\n\u001b[1;32m   1116\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1117\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m as \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m from `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` subfolder of \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m init_kwargs[name] \u001b[39m=\u001b[39m loaded_sub_model  \u001b[39m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SDXL/lib/python3.8/site-packages/diffusers/pipelines/pipeline_utils.py:467\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39m# check if the module is in a subdirectory\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 467\u001b[0m     loaded_sub_model \u001b[39m=\u001b[39m load_method(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(cached_folder, name), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mloading_kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[39m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     loaded_sub_model \u001b[39m=\u001b[39m load_method(cached_folder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloading_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SDXL/lib/python3.8/site-packages/diffusers/models/modeling_utils.py:645\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot load \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m because \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m}\u001b[39;00m\u001b[39m expected shape \u001b[39m\u001b[39m{\u001b[39;00mempty_state_dict[param_name]\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m. If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[39mif\u001b[39;00m accepts_dtype:\n\u001b[0;32m--> 645\u001b[0m     set_module_tensor_to_device(\n\u001b[1;32m    646\u001b[0m         model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, dtype\u001b[39m=\u001b[39;49mtorch_dtype\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, value\u001b[39m=\u001b[39mparam)\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SDXL/lib/python3.8/site-packages/accelerate/utils/modeling.py:293\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    291\u001b[0m         value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mto(old_value\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    292\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mstr\u001b[39m(value\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mstartswith((\u001b[39m\"\u001b[39m\u001b[39mtorch.uint\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.int\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.bool\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m--> 293\u001b[0m         value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(dtype)\n\u001b[1;32m    295\u001b[0m param \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39mif\u001b[39;00m tensor_name \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_parameters \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m param_cls \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(param)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 2\n",
    "cfg = 9.5  # classifier free guidance scale\n",
    "eta = 0.5\n",
    "ddim_steps = 50\n",
    "caption_idx = 2\n",
    "control_scale = 1\n",
    "control_mode = 'midas'  # ('canny', 'midas')\n",
    "path2samples = '/export/data/ffeiden/ResultsControlNetXS/T2I/'  # root for samples\n",
    "model_version = 't2i_SDXL_depth'  # name of model/version\n",
    "sdxl = False\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  \"TencentARC/t2i-adapter-depth-midas-sdxl-1.0\", torch_dtype=torch.float16,\n",
    "  cache_dir='/export/scratch/ffeiden/Pretrained%20Originals/T2I_XL/'\n",
    ").to(\"cuda\")\n",
    "\n",
    "# load euler_a scheduler\n",
    "model_id = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "euler_a = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\",\n",
    "                                        cache_dir='/export/scratch/ffeiden/Pretrained%20Originals/T2I_XL/')\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16,\n",
    "                                    cache_dir='/export/scratch/ffeiden/Pretrained%20Originals/T2I_XL/')\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    model_id, vae=vae, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16,\n",
    "    cache_dir='/export/scratch/ffeiden/Pretrained%20Originals/T2I_XL/'\n",
    "    ).to(\"cuda\")\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "caption_csv_list = [\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx0.json',\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx1.json',\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx2.json',\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx3.json',\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val_idx4.json',\n",
    "    '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_captions_val.json'\n",
    "                    ]\n",
    "data_csv = '/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/coco2017_image_list_val.txt'\n",
    "caption_csv = caption_csv_list[caption_idx]\n",
    "\n",
    "coco_set = lae.LaionBase(\n",
    "    size=512,\n",
    "    random_resized_crop=False,\n",
    "    control_mode='canny',\n",
    "    data_root='/export/data/vislearn/rother_subgroup/dzavadsk/datasets/coco2017/val2017',\n",
    "    full_set=True,\n",
    "    data_csv=data_csv,\n",
    "    caption_csv=caption_csv,\n",
    "    np_format=not sdxl,\n",
    "    original_size_as_tuple=True,\n",
    "    crop_coords_top_left=True,\n",
    "    target_size_as_tuple=True,\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "coco_set.canny_tresholds = np.concatenate([\n",
    "    np.random.randint(50, 100, [len(coco_set), 1]),\n",
    "    np.random.randint(200, 350, [len(coco_set), 1])], axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sd_sample_set(\n",
    "        pipe=pipe,\n",
    "        ds=coco_set,\n",
    "        path2samples=path2samples,\n",
    "        model_version=model_version,\n",
    "        n_samples=n_samples,\n",
    "        scale=cfg,\n",
    "        eta=eta,\n",
    "        ddim_steps=ddim_steps,\n",
    "        caption_idx=caption_idx,\n",
    "        control_scale=control_scale,\n",
    "        control_mode=control_mode,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2I_SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
