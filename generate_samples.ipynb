{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from basicsr.utils import tensor2img\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "\n",
    "from ldm.inference_base import (diffusion_inference, get_adapters, get_base_argument_parser, get_sd_models)\n",
    "from ldm.modules.extra_condition import api\n",
    "from ldm.modules.extra_condition.api import (ExtraCondition, get_adapter_feature, get_cond_model)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.cuda.set_device('cuda:3')\n",
    "import glob\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0 255 ...   3   0   0]\n",
      " [254 253   1 ...   1 255 255]\n",
      " [  0 255 251 ...   0   0 254]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  2   0   0 ...   0   0   0]\n",
      " [  0   1   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "model_edges = Image.open('/export/data/ffeiden/ResultsControlNetXS/T2I/t2i_canny/steps-50/caption-2/000000_depht.jpg')\n",
    "print(np.array(model_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = '/export/data/ffeiden/PaperControlnetXS/512_images/'\n",
    "size = 512\n",
    "\n",
    "\n",
    "paths = glob.glob(image_root+'*_depth*.png')\n",
    "\n",
    "prompts = []\n",
    "for im_path in paths: \n",
    "    images = {}\n",
    "    name = im_path.split('/')[-1]\n",
    "    name, alternative = name.split('_depth')[0], name.split('_depth')[1]\n",
    "\n",
    "    prompts.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class deafult_settings(): \n",
    "    def __init__(self):\n",
    "        self.outdir = '/export/home/ffeiden/Projects/T2I-Adapter/outputs/test_gen' # str\n",
    "        self.prompt = prompts # str\n",
    "        self.neg_prompt = '' # str longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, ' \\\n",
    "                             # 'fewer digits, cropped, worst quality, low quality\n",
    "        self.cond_path = paths # str: condition image path\n",
    "        self.cond_inp_type = 'depth' # str: the type of the input condition image, take depth T2I as example, the input can be raw image, '\n",
    "                                     # 'which depth will be calculated, or the input can be a directly a depth map image\n",
    "        self.sampler = 'ddim' # str: ddim, plms\n",
    "        self.steps = 50 # int: numper of sampling steps\n",
    "        self.sd_ckpt = '/export/data/vislearn/rother_subgroup/dzavadsk/models/pretrained_originals/StableDiffusion/v1-5-pruned.ckpt'\n",
    "                        # str: path to sd ckpt or safetensor\n",
    "        self.vae_ckpt = None # str: VAE checkpoint \n",
    "        self.adapter_ckpt = '/export/data/vislearn/rother_subgroup/feiden/models/pretrained/T2I_Adapter/t2iadapter_depth_sd15v2.pth'\n",
    "                            # str: Adapter ckpt\n",
    "        self.config = 'configs/stable-diffusion/sd-v1-inference.yaml' # str: path to config\n",
    "        self.max_resolution = 512 * 512 # float, max image hight * width\n",
    "        self.resize_short_edge = None # int: resize short edge of the input image, if set max_res not used\n",
    "        self.C = 4 # int: latent channels\n",
    "        self.f = 8 # int: downsampling factor\n",
    "        self.scale = 7.5 # float: unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
    "        self.cond_tau = 1.0 # float: timestamp parameter that determines until which step adapter is applied\n",
    "        self.style_cond_tau = 1.0 # timestamp parameter that determines until which step the adapter is applied\n",
    "        self.cond_weight = 1.0 # float: the adapter features are multiplied with this (control strength)\n",
    "        self.seed = 42 # int\n",
    "        self.n_samples = 1 # int: # of samples to generate\n",
    "        self.which_cond = 'depth' #str:  sketch keypose seg depth canny style color openpose\n",
    "        self.device = 'cuda'\n",
    "\n",
    "opt = deafult_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /export/data/vislearn/rother_subgroup/dzavadsk/models/pretrained_originals/StableDiffusion/v1-5-pruned.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sd_model, sampler = get_sd_models(opt)\n",
    "which_cond = opt.which_cond\n",
    "adapter = get_adapters(opt, getattr(ExtraCondition, which_cond))\n",
    "cond_model = None\n",
    "if opt.cond_inp_type == 'image':\n",
    "    cond_model = get_cond_model(opt, getattr(ExtraCondition, which_cond))\n",
    "\n",
    "process_cond_module = getattr(api, f'get_cond_{which_cond}')\n",
    "\n",
    "\n",
    "def give_params(model):\n",
    "    total_params = sum(\n",
    "                    param.numel() for param in model.parameters()\n",
    "                      )\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return total_params, trainable_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd_model: trainable: 1_066_235_307 ; trainable: 859_520_964\n",
      "adapter_model: trainable: 77_369_280 ; trainable: 77_369_280\n",
      "Trainable_params total: 936_890_244\n"
     ]
    }
   ],
   "source": [
    "total, train = give_params(sd_model)    \n",
    "print(f'sd_model: trainable: {total:_} ; trainable: {train:_}')\n",
    "ad_total, ad_train = give_params(adapter['model'])\n",
    "print(f'adapter_model: trainable: {ad_total:_} ; trainable: {ad_train:_}')\n",
    "\n",
    "print(f'Trainable_params total: {train + ad_train:_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_depth(opt):\n",
    "    which_cond = opt.which_cond\n",
    "    if opt.outdir is None:\n",
    "        opt.outdir = f'outputs/test-{which_cond}'\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    if opt.resize_short_edge is None:\n",
    "        print(f\"you don't specify the resize_shot_edge, so the maximum resolution is set to {opt.max_resolution}\")\n",
    "    opt.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # support two test mode: single image test, and batch test (through a txt file)\n",
    "    if type(opt.prompt) is not list:\n",
    "        if opt.prompt.endswith('.txt'):\n",
    "            assert opt.prompt.endswith('.txt')\n",
    "            image_paths = []\n",
    "            prompts = []\n",
    "            with open(opt.prompt, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    image_paths.append(line.split('; ')[0])\n",
    "                    prompts.append(line.split('; ')[1])\n",
    "    else:\n",
    "        image_paths = opt.cond_path\n",
    "        prompts = opt.prompt\n",
    "    print(image_paths)\n",
    "\n",
    "    # prepare models\n",
    "    sd_model, sampler = get_sd_models(opt)\n",
    "    adapter = get_adapters(opt, getattr(ExtraCondition, which_cond))\n",
    "    cond_model = None\n",
    "    if opt.cond_inp_type == 'image':\n",
    "        cond_model = get_cond_model(opt, getattr(ExtraCondition, which_cond))\n",
    "\n",
    "    process_cond_module = getattr(api, f'get_cond_{which_cond}')\n",
    "\n",
    "    # inference\n",
    "    with torch.inference_mode(), \\\n",
    "            sd_model.ema_scope(), \\\n",
    "            autocast('cuda'):\n",
    "        for i in range(len(image_paths)):\n",
    "            cond_path, prompt = image_paths[i], prompts[i]\n",
    "            seed_everything(opt.seed)\n",
    "            for v_idx in range(opt.n_samples):\n",
    "                # seed_everything(opt.seed+v_idx+test_idx)\n",
    "                cond = process_cond_module(opt, cond_path, opt.cond_inp_type, cond_model)\n",
    "\n",
    "                base_count = len(os.listdir(opt.outdir)) // 2\n",
    "\n",
    "                adapter_features, append_to_context = get_adapter_feature(cond, adapter)\n",
    "                opt.prompt = prompt\n",
    "                result = diffusion_inference(opt, sd_model, sampler, adapter_features, append_to_context)\n",
    "                cv2.imwrite(os.path.join(opt.outdir, f'{i:05}_result.png'), tensor2img(result))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you don't specify the resize_shot_edge, so the maximum resolution is set to 262144\n",
      "['/export/data/ffeiden/PaperControlnetXS/512_images/render of a lavender cube on red background_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Image of mickey mouse standing and smiling_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Photo of an empty street with cars parked on both sides_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/render of a green cube on NavyBlue background_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/close up image of a face, manga style, blue hair_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Photo of a woman wearing a summer dress and a hat_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Portrait of a thoughtful young woman, photography, 4k_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Photo of a big house with stores at the first floor, cars parked, 4k_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/drone image of a skyscraper in a city at night_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/render of a LawnGreen cube on light blue background_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/aerial image of a city with a big highway intersection_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Image of mickey mouse standing and smiling_depth_alt.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Aerial image of a beach with three small boats_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Photograph of a street with cars, city, cloudy_depth.png', '/export/data/ffeiden/PaperControlnetXS/512_images/Professional photo of a man wearing a suit_depth.png']\n",
      "Loading model from /export/data/vislearn/rother_subgroup/dzavadsk/models/pretrained_originals/StableDiffusion/v1-5-pruned.ckpt\n",
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "Data shape for DDIM sampling is (1.0, 4, 64, 64), eta 0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "randn() received an invalid combination of arguments - got (tuple, device=torch.device), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_depth(opt)\n",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mrun_depth\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     49\u001b[0m adapter_features, append_to_context \u001b[39m=\u001b[39m get_adapter_feature(cond, adapter)\n\u001b[1;32m     50\u001b[0m opt\u001b[39m.\u001b[39mprompt \u001b[39m=\u001b[39m prompt\n\u001b[0;32m---> 51\u001b[0m result \u001b[39m=\u001b[39m diffusion_inference(opt, sd_model, sampler, adapter_features, append_to_context)\n\u001b[1;32m     52\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(opt\u001b[39m.\u001b[39moutdir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m05\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_result.png\u001b[39m\u001b[39m'\u001b[39m), tensor2img(result))\n",
      "File \u001b[0;32m~/Projects/T2I-Adapter/ldm/inference_base.py:282\u001b[0m, in \u001b[0;36mdiffusion_inference\u001b[0;34m(opt, model, sampler, adapter_features, append_to_context, batch_size)\u001b[0m\n\u001b[1;32m    279\u001b[0m     opt\u001b[39m.\u001b[39mW \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m    280\u001b[0m shape \u001b[39m=\u001b[39m [opt\u001b[39m.\u001b[39mC, opt\u001b[39m.\u001b[39mH \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m opt\u001b[39m.\u001b[39mf, opt\u001b[39m.\u001b[39mW \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m opt\u001b[39m.\u001b[39mf]\n\u001b[0;32m--> 282\u001b[0m samples_latents, _ \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    283\u001b[0m     S\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49msteps,\n\u001b[1;32m    284\u001b[0m     conditioning\u001b[39m=\u001b[39;49mc,\n\u001b[1;32m    285\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    286\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    287\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    288\u001b[0m     unconditional_guidance_scale\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mscale,\n\u001b[1;32m    289\u001b[0m     unconditional_conditioning\u001b[39m=\u001b[39;49muc,\n\u001b[1;32m    290\u001b[0m     x_T\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    291\u001b[0m     features_adapter\u001b[39m=\u001b[39;49madapter_features,\n\u001b[1;32m    292\u001b[0m     append_to_context\u001b[39m=\u001b[39;49mappend_to_context,\n\u001b[1;32m    293\u001b[0m     cond_tau\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mcond_tau,\n\u001b[1;32m    294\u001b[0m     style_cond_tau\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mstyle_cond_tau,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m x_samples \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode_first_stage(samples_latents)\n\u001b[1;32m    298\u001b[0m x_samples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp((x_samples \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SD/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/T2I-Adapter/ldm/models/diffusion/ddim.py:99\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, features_adapter, append_to_context, cond_tau, style_cond_tau, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m size \u001b[39m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m     97\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData shape for DDIM sampling is \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, eta \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m samples, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_sampling(conditioning, size,\n\u001b[1;32m    100\u001b[0m                                             callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    101\u001b[0m                                             img_callback\u001b[39m=\u001b[39;49mimg_callback,\n\u001b[1;32m    102\u001b[0m                                             quantize_denoised\u001b[39m=\u001b[39;49mquantize_x0,\n\u001b[1;32m    103\u001b[0m                                             mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0,\n\u001b[1;32m    104\u001b[0m                                             ddim_use_original_steps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    105\u001b[0m                                             noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[1;32m    106\u001b[0m                                             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    107\u001b[0m                                             score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[1;32m    108\u001b[0m                                             corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[1;32m    109\u001b[0m                                             x_T\u001b[39m=\u001b[39;49mx_T,\n\u001b[1;32m    110\u001b[0m                                             log_every_t\u001b[39m=\u001b[39;49mlog_every_t,\n\u001b[1;32m    111\u001b[0m                                             unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    112\u001b[0m                                             unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[1;32m    113\u001b[0m                                             features_adapter\u001b[39m=\u001b[39;49mfeatures_adapter,\n\u001b[1;32m    114\u001b[0m                                             append_to_context\u001b[39m=\u001b[39;49mappend_to_context,\n\u001b[1;32m    115\u001b[0m                                             cond_tau\u001b[39m=\u001b[39;49mcond_tau,\n\u001b[1;32m    116\u001b[0m                                             style_cond_tau\u001b[39m=\u001b[39;49mstyle_cond_tau,\n\u001b[1;32m    117\u001b[0m                                             )\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniconda3/envs/T2I_SD/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/T2I-Adapter/ldm/models/diffusion/ddim.py:131\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, features_adapter, append_to_context, cond_tau, style_cond_tau)\u001b[0m\n\u001b[1;32m    129\u001b[0m b \u001b[39m=\u001b[39m shape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m x_T \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandn(shape, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     img \u001b[39m=\u001b[39m x_T\n",
      "\u001b[0;31mTypeError\u001b[0m: randn() received an invalid combination of arguments - got (tuple, device=torch.device), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "run_depth(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2I_SD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
